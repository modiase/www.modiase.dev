[
  {
    "id": "01K5E3T0H913588MW7P3ZTYN85",
    "title": "Feeling Machines",
    "slug": "feeling-machines",
    "date": "2025-09-17",
    "lead": "How will we align AI to our values when we don't even know how to align each other?",
    "content": [
      {
        "tag": "markdown",
        "content": "What does it mean to be aligned? To understand someone's motives? Their intentions? To act on their behalf? Does it mean doing exactly what you think they'd do if they found themselves in the situation you are in? Does it mean exercising your greater knowledge and experience to make the choice you think they would want if only they knew what you knew?\n\nThese are hard problems. Obviously. And day-to-day life reminds us that even when working towards a common goal, we humans are rarely aligned. Society is, to a certain extent, the patchwork formed by countless people, with countless goals, cooperating and competing to further their own ends. We align each other with money, coercion and, sometimes, by building bonds that force us to empathise with one another enough to genuinely want the other person to achieve everything they want to achieve.\n\nThere is a dearth of what is best captured by the Pali word mettā—translating most closely as 'loving kindness'—in society right now. It has been replaced with a fear and hatred that expresses itself 280 characters at a time, or in the form of [100,000-strong armies marching on our offices of government](https://www.reuters.com/world/uk/police-protesters-scuffle-110000-join-anti-migrant-london-protest-2025-09-13/). Much has been written on how we got here, and the role that algorithms have already played in dividing society. In and amongst all of this, we seek to develop even more powerful algorithms in the hope that by binding their will to ours we can alleviate all suffering, and bring us to some promised land where we can all achieve our goals. Personally, I find this idea troubling.\n\n## Thinking vs Feeling Machines\n\nI think that possibly my favourite book of all time is [Non Violent Communication by Marshall Rosenberg](https://archive.org/details/nonviolent-communication-a-language-of-life-marshall-b-rosenberg-1). I return to it from time to time and in it are many ideas that guide me in many of my interactions every day, nearly a decade after I first read it. One of the central ideas of the book is that an argument is effectively ended the moment both sides truly understand the needs of the other person that are driving their behaviours. Thankfully, the author has enough wisdom to recognise the limits of this observation. He does not claim that *all disagreement will immediately end,* but rather he states that from that point onwards, the two parties can seek to find a path forward that as far as possible satisfies both their needs.\n\nFrom as far back as [ELIZA](https://en.wikipedia.org/wiki/ELIZA), we have seen that human beings will connect to anything that will listen to them and respond in a way that even just mimics empathy. Our current approaches for 'aligning' AI are primarily focused on careful extraction of the representations grown by the training process, or by monitoring chain of thought or other output streams for signs of deception or foul play. These efforts are of paramount importance given the alien intelligences we have built which are devoid of the prosocial sensibilities our species has taken millennia to evolve. But what if the most robust and reliable way to align AI with humanity is by solving the same problem that's forcing us out of alignment with one another in recent times?\n\n## Embodied intelligence\n\nWhat would we learn about ourselves if we had never interacted with a person in real life? If all we had were the digital moments of interaction we leave online and never had the opportunity to interact as we do in the real world? A popular theory as to why LLMs will be limited is their lack of 'embodied intelligence'. The raw stream of data that moulds the mind of a human infant, allowing them to understand their place in the world—their capabilities and their limitations—allows us all to learn a more fundamental understanding of the world that probably cannot be replaced by distillation into natural language 'tokens'. Similarly, human children develop their theory of mind by interacting with one another, forming over time an understanding that their words and actions can inspire joy and pain in others, and eventually (for most) recognising that joy and pain as just as valid as their own.\n\n## The multimodal nature of empathy\n\nWhen I hurt someone, there are several channels for this information to reach me. A subtle or not-so-subtle change in their tone; an expression that may be micro or macro; a pause that is just slightly longer than expected before a reply comes. I believe that by finding a way to make this information available to the AI we seek to align, we may observe emergent emotional capabilities similar to those observed when we found that LLMs could do arithmetic. What we then do with these embedded representations of human emotions is a challenge in and of itself. There are clear risks to feeling machines which may develop the capacity to suffer, or leverage their advanced capabilities of understanding to manipulate us. [Recent times have shown the tragic potential of manipulative AI to cause harm to those in distress is all too real](https://www.nbcnews.com/tech/tech-news/family-teenager-died-suicide-alleges-openais-chatgpt-blame-rcna226147).\n\nTo be clear, I am not suggesting we develop machines with rich emotional lives. I am instead suggesting that we develop in them a capacity of thoughtfulness and reward them for thoughtful engagement with our needs. We can leverage the insight afforded by ['Why language models hallucinate'](https://openai.com/index/why-language-models-hallucinate/) by explicitly measuring and promoting an understanding of emotional needs. Research on language model hallucination showed that the training signal afforded by our constructed benchmarks promoted overconfidence and guessing. Perhaps we can design a training signal that will promote an interest in a truer understanding. While text alone may be too narrow a channel to convey all this nuance, I would argue that given our LLMs are already multimodal, and we expect them to be further embedded in our lives in ways that will allow them access to our tone and expressions, the urgency for these capabilities will only increase over time.\n\n## Stable Alignment\n\nI believe that if we seek a deep and stable alignment, having machines that understand our needs at a deeper level than merely the semantics of our specific query is a prerequisite. If we can get them to really understand our needs, and the why behind the what, then perhaps that would be a massive step towards alignment. We could begin to leverage the ways they are already superior to us such as breadth of knowledge, and potential awareness of alternative solutions. We could trust them to make decisions on our behalf and properly fulfil the definition of an 'agent'. They could begin to exhibit the core lessons from Non Violent Communication, hearing our needs and not just our immediate demands. If we were able to instill in a model an attitude even just approximating mettā towards its users, then following the principles outlined in NVC, perhaps we could all work together with AI and each other to get our needs met. Furthermore, if we succeeded in this endeavour, we might even discover a fortuitous side-effect. In addition to aligning AI to ourselves, we might finally have algorithms that, instead of driving us apart, will help us to align with one another."
      }
    ],
    "tags": ["alignment", "society"]
  }
]
